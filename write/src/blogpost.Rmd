---
title: Processing scanned documents for human rights investigations
author:
- "[Tarak Shah](https://hrdag.org/people/tarak-shah/)"
output:
    github_document:
        toc: true
---

```{r opts, echo = FALSE}
knitr::opts_knit$set(root.dir = here::here("write"))
knitr::opts_chunk$set(fig.path = "output/")
```


# Introduction and background

Administrative paperwork generated by abusive regimes have long been an
integral component of human rights investigations. For an international
example, see HRDAG's 2010 report [State Coordinated Violence in Chad under
Hissène
Habré](https://hrdag.org/wp-content/uploads/2013/02/State-Violence-in-Chad.pdf).
In that report, investigators used analysis of documents found in an abandoned
state security force (DDS) headquarters in N'Djamena in order to demonstrate
systemic human rights violations in DDS prisons and attribute command
responsibility to President Hissène Habré.

In the U.S. context, in conjunction with other data sources, documents obtained
through public records law can provide important evidence of state violence as
well as the paper trail to demonstrate responsibility for human rights
violations. Among the many formats such records can take, scanned and redacted
documents saved as PDF files are common, and present unique challenges for data
processing. In this post, I review the methods and software tools we use to
process such collections and get them into an analysis-ready format.

## Running examples

- The University of Washington Center for Human Rights (UWCHR) [sued under the
  Freedom of Information
  Act](https://jsis.washington.edu/humanrights/2018/09/21/uwchr-sues-dhs-ice-cbp/)
  for every form I-213 (recording apprehensions of people) produced by ICE or
  CBP in the Washington area for a number of years as part of the Center's
  [Human Rights at
  Home](https://jsis.washington.edu/humanrights/projects/human-rights-at-home/)
  research regarding immigrant rights.

- The ACLU of Massachusetts is reviewing Boston PD SWAT reports
  (these are the reports filled out before and after tactical and warrant
  service operations) made public under the [17F
  order](https://www.wgbh.org/news/local-news/2020/06/11/boston-city-council-moves-to-request-information-about-boston-police-departments-military-equipment).

- The [Invisible Institute](https://invisible.institute/police-data) has
  been processing hundreds of thousands of pages of complaints against police
  and associated investigative fiels released through the court case *Green v.
  Chicago*.

## Project structure

At HRDAG, we organize our projects as collections of self-contained tasks, as
described by my colleague Dr. Patrick Ball [in this
blogpost](https://hrdag.org/2016/06/14/the-task-is-a-quantum-of-workflow/) and
in [this lecture](https://youtu.be/ZSunU9GQdcI). Here is a task breakdown from
a recent project (with names and ordering modified for clarity), which we'll
use as a running outline:

```
individual/foia-xyz
├── import
│   ├── index
│   ├── ocr
│   └── export
├── classify-pages
│   ├── import
│   ├── sample
│   ├── features
│   ├── train
│   ├── classify
│   └── export
├── recover-page-structure
│   ├── import
│   ├── sample
│   ├── segment
│   ├── features
│   ├── train
│   ├── classify
│   └── export
├── extract
│   ├── import
│   ├── parse
│   ├── clean
│   ├── calculated-fields
│   ├── topic-models
│   └── export
└── ...
```

Each data processing step is framed as a classification problem, and the
classification tasks are well-suited to machine learning solutions.

Within tasks that require labeled data, the `import` step should concatenate
and pre-process any hand labeled data, in addition to bringing in the output(s)
from the previous step in the pipeline (the latter can be as minimal as
creating a symlink in the output directory).

## Sample, review, label

At each step in the processing pipeline, we design a way to sample from the
document collection. Randomly sampling and reviewing is a disciplined way to
learn about the collection, as opposed to manually flipping through a few pages
or just looking at the first few pages of each file, or a similar method. At
the start, "review" can be informal. But to act on the information we learn
through review, we need to convert our notes to a [controlled
vocabulary](https://hrdag.org/controlled-vocabulary/) -- this is the labeling
step.

This approach is not unique to digitized collections. See HRDAG's work with the
[Guatemalan National Police Archive
Project](https://hrdag.org/guatemalan-national-police-archive-project/) for an
example of sampling physical documents laid out in piles strewn throughout a
building. In a paper from 2013, [Price et al describe coding the sampled
documents](https://hrdag.org/wp-content/uploads/2013/02/JSM-GT-estimates.pdf)
for presence of each of 11 different acts of interest: detentions, deaths,
denunciations, physical and psychological abuse, intimidation, disappearances,
writ of habeas corpus, interrogations, entering private property, sexual abuse,
and surveillance. This example shows how the correct set of codes to use for a
given project are not inherent to the document collection, but rather depend on
the analytical questions we hope to address by analyzing the collection.

That example demonstrates the use of the sample-review-label pattern to
understand the content of extracted data. As we'll see in this post, we use the
same pattern to inform structured information extraction from scanned pages.

Though our sampling relies on randomness, we're not necessarily trying to
collect a representative sample of the entire database. Instead, we're trying
to maximize the variety and informativeness of our overall labeled dataset. As
we create additional labels, we become better able to target records for review
that would provide the most information value -- for instance by sampling just
records for which our ML classifier scores between .4 and .6. or sampling in a
way that over-selects the minority class in order to guarantee a healthy mix of
positive and negative examples. For more about targeting training samples, see
[this blogpost on "active
learning"](https://humanloop.com/blog/why-you-should-be-using-active-learning/).

## Heuristic and machine learning approaches complement each other

The approach outlined here is meant to facilitate the use of machine learning
solutions for each stage of data processing. Sometimes, you can accomplish the
classification task more readily using hand-rolled logic (aka a "heuristic"
classifier) -- for instance classifying pages by checking the header and footer
for a handful of regular expressions. In such cases, it's still valuable to
organize code as I have here. You'll still want to sample and label some
examples, although for acceptance tests rather than training data. And it's
still valuable to split out the `features` code from the `classify` step that
scores/classifies all records in the database. That's helpful for managing code
complexity. And it makes it straightforward to swap in an ML solution if the
logic starts getting unwieldy (and your ML classifier immediately benefits from
the detailed hand-crafted features you designed for heuristic classification).

Indeed, a pattern that has worked often for me is to start out with a heuristic
classifier before I have any training data. I can usually come up with
something semi-decent based on the documents I've reviewed. Then when I sample
data for labeling (to evaluate classifier performance), I include the output of
the heuristic classifier. Labeling data goes more quickly, since instead of
typing every label I just have to correct the ones that the heuristic model got
wrong.  It's helpful to continue "pre-filling" the labeling data with the
output of the best current classifier in this way to speed up additional data
collection as your classifier improves. For a more principled approach to
utilizing heuristics to produce training data, check out
[Snorkel](https://www.snorkel.org/).

# Initial Steps

```
...
├── import
│   ├── index
│   ├── ocr
│   └── export
...
```

## Creating an index

Before processing any files, we always create a metadata table along with a
stable identifier for each file. We use a sha1 hash to confirm file identity and
truncate it for a conveniently sized file identifier:

```{r, eval=FALSE}
pacman::p_load(digest, qpdf, tidyverse)

inputfiles <- list.files(INPUTDIR, full.names=TRUE, pattern='*.pdf')
index <- tibble(filename = inputfiles) %>%
    mutate(filesha1 = map_chr(filename, digest, algo='sha1', file=TRUE),
           fileid   = str_sub(filesha1, 1, 8),
           n_pages  = map_int(filename, pdf_length))

stopifnot(
    "files should be unique" = length(unique(index$filesha1)) == nrow(index),
    "fileids should be unique" = length(unique(index$fileid)) == nrow(index),
)

write_delim(index, OUTPUTNAME, delim='|')
```

The index should include any additional metadata available about the files. For
instance, if files arrived in batches, we include the batch number. Often
metadata is encoded in filenames (for example files all named
'cases-yyyymm.pdf'), in which case we extract that metadata into an explicit
column(s) in the index.

## OCR

We use [tesseract](https://tesseract-ocr.github.io/) for doing OCR, often via
the [tesseract](https://github.com/ropensci/tesseract) R package. For mixed
format documents, we use the position and size of words on the page, along with
the content, in order to accurately process them. For that reason, we use
Tesseract's option to output data in the [`hOCR`
format](http://kba.cloud/hocr-spec/1.2/).

The [tesseract
documentation](https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html)
goes into some detail about what image pre-processing is and isn't already
performed by Tesseract. For the types of documents we're looking at, the most
helpful pre-processing steps involve removing lines from forms, and painting
over redaction rectangles with white space (both of which improve text
recognition). Helpful resources:

- [Stackoverflow Question about removing
  lines/borders](https://stackoverflow.com/questions/33949831/whats-the-way-to-remove-all-lines-and-borders-in-imagekeep-texts-programmatic)
- [Blogpost about image preprocessing for OCR-ing tables in R using the
  `magick`
  package](https://themockup.blog/posts/2021-01-18-reading-tables-from-images-with-magick/)

# Page classification

```
...
├── classify-pages
│   ├── import
│   ├── sample
│   ├── features
│   ├── train
│   ├── classify
│   └── export
...

```

## Examples

Recently, the Invisible Institute's CPDP documents processing team faced a
[deluge of documents released through the *Green v. Chicago*
lawsuit](https://invisible.institute/police-data).  Consisting of
investigations of Chicago Police misconduct, the collection contains hundreds
of thousands of pages of allegation forms, memos, various police administrative
forms, interviews and testimonies, pictures, and even embedded audio files.

One of the CPDP team's goals was to publish the documents on
[cpdp.co](https://cpdp.co/).  However, many of the documents contain
identifying information about survivors and witnesses. In order to publish
responsibly, the team sought to first organize the documents into document
types, and then identify document types that are known to be free of
identifying information. For instance, each complaint is associated with a
summary "Face Sheet" that is safe to publish.  CPDP volunteer Phil put together
a neural network based classifier that took in both page images and OCR text
and classified each document as one among a sizable universe of known document
types.

At the other end of the spectrum, the University of Washington Center for Human
Rights had FOIA'd for every form I-213 (recording apprehensions of people)
produced by ICE or CBP in the Washington area for a number of years. Here all
documents were of the same type, but they arrived in a handful of
multi-thousand page PDFs. The I-213 varies in page length depending on the
length of the written narrative, so we didn't know where each document began
and ended. We wrote code to classify each page as "front" or "continuation." It
was only after we'd done page classification that we could even answer a
question as basic as "how many documents did we receive?" (a continuing insight
from working at HRDAG is that counting things is hard).

## A basic page sampler

Between `qpdf::subset_pdf` and `qpdf::pdf_combine`, we can sample pages from a
variety of files, and combine them into one PDF for convenient review:

```{r, eval=FALSE}
pacman::p_load(qpdf, tidyverse)

sampled <- index %>%
    distinct(fileid, filename, pg) %>%
    nest(data   = -fileid) %>%
    mutate(data = map(data, sample_n, SAMPSIZE)) %>%
    unnest(data)

partials <- sampled %>%
    mutate(input  = filename,
           output = paste0(OUTPUTDIR, '/', fileid, '.pdf')) %>%
    group_by(fileid, input, output) %>%
    summarise(pg=list(pg), .groups='drop') %>%
    mutate(done = pmap_chr(., subset_pdf))

outfile <- paste0(OUTPUTDIR, "/combined-sample.pdf")
combined <- qpdf::pdf_combine(partials$done, outfile)
```

This code uses stratified sampling, sampling per file. We expect some of the
variation in the look and content of pages to be captured by differences in
file, and this way of sampling allows us to sample equally from longer and
shorter files. Again, if we have access to additional metadata that we think
would be useful in categorizing pages, we can add logic to our sampling code to
ensure a diverse sample.

This is enough to start to get a feel for an unwieldy collection. Every time
I've done this, I've found something that surprised me: random handwritten
notes tucked between documents, rotated or upside down pages, unexpected
landscape pages, document types that weren't part of the request, etc.

## Designing a classifier: labels

Once you've seen enough to be able to identify one or more page types that you
want to label, you can modify the above script to also output a spreadsheet
with the original fileid and page number plus the page number in the sampled
pdf. That makes collecting ground-truth data about page types a breeze, and you
can use the collected data for acceptance tests for heuristic classifiers or as
training data for ML classifiers.

Unless the types of documents in the collection are known to be constrained to
one or two types (as in the UW-CHR example), sampling in this way reveals a
bewildering heterogeneity. Start out by focusing on a handful of page and
document types that are known to be of investigative interest, and focus on
labeling pages as one of those types, or "other." That can be valuable in
itself, and by shaving off the more common and easily recognized types, you
gain better visibility into the more (formally) exceptional pages in the
collection.

In creating the page classifier for the Green documents, it was difficult to
even figure out what the universe of document types were -- each new sample
produced new document types that hadn't been seen before. However, the team was
able to identify and label a couple dozen known document types and train a
classifier. From there, Phil noticed that he could do K-means clustering using
the document type scores -- documents that got clustered together were always
of the same administrative document type, even if that type had not been seen
or labeled before! In this way, he was able to efficiently target new training
samples for labeling, resulting in a quickly improving classifier that covered
more and more of the documents in the collection.

## The `docid`

By the end of page classification, we are able to assign a new kind of
identifier. A `docid` uniquely identifies a *document*, which can span multiple
pages across one or more files. To count how many documents are in the
collection, we count unique `docid`s.

From here on out, we'll sample entire documents rather than individual pages.
The document is a much more meaningful unit of analysis for non-technical
subject matter experts, and so this is a good time to produce some samples for
them to review.

# Recovering structure from document layouts

```
...
├── recover-page-structure
│   ├── import
│   ├── sample
│   ├── segment
│   ├── features
│   ├── train
│   ├── classify
│   └── export
...
```

Administrative documents can consist of a bunch of different types of content,
including:

- headers
- footers
- form elements with standard field labels and typed or handwritten values
- tables with rows and columns
- single and multi-level lists
- ad hoc form elements and tables (laid out manually, e.g. in MS Word)
- section headings, subheadings, titles
- natural language text in sentences and paragraphs

In order to extract useful data, we need to be able to zero in on where that
data is physically on the page. Tools like
[layout-parser](https://github.com/Layout-Parser/layout-parser) and [Amazon
Textract](https://aws.amazon.com/textract/) may prove helpful here.

## Examples

Investigators with the Invisible Institute's ongoing *Beneath the Surface*
investigation into gender violence (see the report ["Shrouded in
Silence"](https://www.interruptingcriminalization.com/breaking-the-silence) for
more context) have been reading written reports associated with Chicago Police
misconduct investigations in order to identify incidents of assault that are
not otherwise visible in structured data (because of the way such incidents are
coded). They had been doing this research by FOIA-ing for documents one
incident at a time until the *Green v. CPD* data provided them, for the first
time, with a collection of tens of thousands of such reports.

We already had access to structured data associated with these investigations
in analyzable formats through [cpdp.co](https://cpdp.co/). So for *Beneath the
Surface* we focused on just extracting written descriptions of allegations from
the documents, rather than a more exhaustive scrape. Utilizing the
aforementioned page classifications, we identified a number of report types
that include descriptions of allegations, and then created heuristic
classifiers to classify each line of text as "allegation description" or "other."

In contrast, the ACLU of Massachusetts has been reviewing Boston PD SWAT after
action incident reports made public under the [17F
order](https://www.wgbh.org/news/local-news/2020/06/11/boston-city-council-moves-to-request-information-about-boston-police-departments-military-equipment).
For this project, we needed to support a wide variety of queries and
investigations, and so sought to extract a large number of data elements from
different document types and combine them into a standard schema. To facilitate
this extraction, we classified pre-segmented regions of text using categories
such as "section header", "table header", "table row", "form element" and so on
(there were between 12 and 46 such categories for different document types).

## Segmenting the page into homogenous regions

In terms of page structure, ideally we'd like a classifier to label each word
on the page with an element identifier (that groups together words that belong
to the same element, such as a multi-word form label) and the category
associated with that element ("table heading", "form element", etc.). In
practice, it's a lot easier to use some layout analysis to pre-group words into
elements and design a classifier to work on those pre-grouped elements.

Often, horizontal lines of text spanning the width of the page are an
appropriate unit for labeling. In fact, for some documents where layout does
not convey that much information (often the case with court transcripts and
letters, for example), we can just work with lines of text without ever
considering bounding boxes.

Other times, it's useful to cluster words based on their position. For
instance, notice how we're able to simplify our task through clustering for
this example from the ACLU Boston PD militarization project. The green boxes on
the right side show the groupings that resulted from clustering:

```{r, echo=FALSE, warning=F, message=F}
pacman::p_load(arrow, tidyverse, patchwork)
samp <- read_parquet("input/segment-layout-data.parquet")
sampdesc = paste0("example: file ", unique(samp$fileid), " page ", unique(samp$pg))
blocks <- samp %>%
    group_by(blockid) %>%
    summarise(x0=min(x0), x1=max(x1), y0=min(y0), y1=max(y1), .groups="drop")

ros <- samp %>%
    group_by(rowid) %>%
    summarise(x0=min(x0), x1=max(x1), y0=min(y0), y1=max(y1), .groups="drop")

plot1 <- ggplot( samp, aes(xmin=x0,xmax=x1,ymin=y0,ymax=y1)) +
    geom_rect(color="black", fill="grey85", size=.1) +
    scale_y_reverse() + coord_equal() +
    theme_void() + theme(legend.position="none") +
    ggtitle(sampdesc)

plot2 <- ggplot( samp, aes(xmin=x0,xmax=x1,ymin=y0,ymax=y1)) +
    geom_rect(data=blocks, colour="black", size=.5, fill="green", alpha=.3) +
    scale_y_reverse() + coord_equal() +
    theme_void() + theme(legend.position="none")

plot1 + plot2
```

We went on to classify each group of words as one of "administrative info,"
"warrant info," and so on. It's much easier to classify groupings like this,
creating features based on presence or absence of specific words in the group,
than to work at the level of word or line. For more on working with layout
data, see the Appendix to this post.

## Modeling tools for sequence labeling

An important insight about the classification task for this stage of the
pipeline is that both the inputs and the outputs are *sequential*, there are
significant correlations between adjacent labels (think of how a "table header"
line is nearly always followed by a "table row"). We get much better results
when using [modeling tools that can represent this kind of dependency
structure](http://web.engr.oregonstate.edu/~tgd/publications/mlsd-ssspr.pdf)
such as [Hidden Markov
Models](https://web.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf)
and [Recursive Neural
Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).

Given the specifics of our situation -- small amounts of training data, rich
handcrafted features, and predictors that combine both layout/positional
information as well as text content, Conditional Random Fields models have
become our go-to choice for this step in data processing.  [Hannah Wallach's
website](https://www.inference.org.uk/hmw26/crf/) contains a number of useful
references about CRF models, I particularly appreciated these papers about
structured extraction from documents using CRF's:

- [Comparing Machine Learning Approaches for Table Recognition in Historical
  Register Books](https://arxiv.org/abs/1906.11901)
- [Table Extraction Using Conditional Random
Fields](https://people.cs.umass.edu/~mccallum/papers/crftable-sigir2003.pdf)

## Leave-one-document-out cross-validation

Though we're labeling at the level of individual lines or other pre-segmented
groups of words, our samples are at the level of entire document. Furthermore,
it's helpful to start with small batches of training data, and target new
samples (for instance by selecting for labeling those documents where the
classifier has low certainty about its outputs) as necessary based on
performance on held-out data. To make the most of small amounts of training
data, during training and parameter tuning I use Leave One Document Out (LODO)
cross-validation. The ability to work with nested list-columns in R facilitates
this kind of grouped sampling. The following code can be extended to iterate
over a grid of parameter values to identify the best ones:

```{r, eval=FALSE}
pacman::p_load(crfsuite, tidyverse, rsample)

cv <- nest(trainfeats, data=-docid) %>%
    loo_cv %>%
    mutate(train = map(splits, training),
           test  = map(splits, testing))

results <- cv %>%
    mutate(model      = map(train, crf_fit),
           test_preds = map2(model, test, crf_predict)
           metrics    = map2(test, test_preds, crf_metrics))
```

The `crf_*` functions are helper functions, `crf_fit` and `crf_predict` are
wrappers for the associated functions in the
[`crfsuite`](https://CRAN.R-project.org/package=crfsuite) package, whereas
`crf_metrics` takes ground truth and predicted labels and tallies the four
quadrants of the [confusion
matrix](https://en.wikipedia.org/wiki/Confusion_matrix), which are then used to
produce evaluation statistics for each set of candidate parameters.

# Data extraction from segmented documents

Having segmented a collection of scanned pages into documents, and divided
documents into their constituent sections, we move to pulling data into a
standardize schema. In the parsing step, we dispatch tables to table-extraction
code (see appendix on working with layout data), convert detected form elements
into key-value pairs, process lists and unstructured text, and so on. These
extracted data become the building blocks for an analysis-ready dataset, but
getting there still requires additional work.

```
...
├── extract
│   ├── import
│   ├── parse
│   ├── clean
│   ├── calculated-fields
│   ├── topic-models
│   └── export
...
```

## Examples

The Boston PD SWAT reports came in multiple layouts, and we designed a software
tool that exhaustively pulled out and parsed all data from each document into a
separate JSON file. From that point, we needed a lot of custom logic in order
to produce a dataset that could drive a human rights investigation.

For example, given the large number of firearms and other military equipment
that the Boston PD document bringing to each raid (which we know because we
extracted those tables), we wanted to be able to identify and flag incidents
where the SWAT team encountered children upon entering a home. This information
was encoded in a variety of different places and ways.

Some reports included a column for age in the table documenting people
encountered, so we were able to create an indicator for whether any of the
people was under 18.  Other documents did not include such a column, or instead
recorded date of birth, which was redacted. Additionally, in the planning
section documenting the decisions leading up to the raid, many of the reports
included an indicator of whether the team expected to encounter children during
the raid (in the section "Potential Hazards"). That section was inconsistent,
and when it was used it was almost always "Unknown" (a remarkable observation
itself!). Still, we were able to identify some additional records that way.
Finally, all reports included a written narrative, which sometimes mentioned
children encountered.  We used some regular expression searches to identify
these narratives.

Altogether, we had to look in 3 different places and use some fairly detailed
logic in order to create a single true/false indicator column in our
standardized output. We went through similar processes to create indicators for
the presence of various pieces of military equipment, the use of stun grenades
and chemical agents, warrant indicators (e.g. whether a raid was for service of
a drug warrant), and the work continues.

Notice that this step in data processing once again lends itself to sampling,
reviewing, and labeling. What I've described are heuristic models for document
classification, and indeed with the ACLU of Massachusetts we're currently
reviewing sampled data to evaluate the accuracy of our classifiers (and to use
as training data, if we go that route).

For *Beneath the Surface*, Trina Reynolds Tyler of the Invisible Insitute has
assembled a team of community organizers, human rights experts, and other
community members to read and code extracted allegation descriptions from
police misconduct documents. For this project we're using topic modeling to
create features from the unstructured allegation text, with the goal of
developing a classifier that can aid human rights investigators in sifting
through the hundreds of thousands of pages of Chicago police misconduct data,
using the methods that [my colleague Patrick Ball and I described in this
earlier
blogpost](https://hrdag.org/tech-notes/indexing-selectors-from-collection-chat-messages.html)

# Conclusions/next steps

# Acknowledgements

CPDP/II Team: Trina Reynolds Tyler, Rajiv Sinclair, Chaclyn Hunt, Matt Chapman,
and CPDP volunteers

UW-CHR Team: Phil Neff, Maria Gargiulo

BPD Team: Aaron Boxer, Lauren Chambers, Kade Crockford

and ongoing discussions with my colleagues Drs. Patrick Ball and Megan Price

# Appendix: Working with layout data

```{r, echo=FALSE, warning=FALSE, message=FALSE}
pacman::p_load(tidyverse, arrow)

gg <- function(data) {
    ggplot(data, aes(xmin=x0, xmax=x1, ymin=y0, ymax=y1)) +
        scale_y_reverse() + coord_fixed(ratio=1) +
        theme_void() + theme(legend.position="none")
}

boxes <- read_parquet("input/table.parquet") %>% rename(id=word_id) %>%
    filter(x0 < 6500)
```

Layout data includes both content (text), along with bounding boxes for each
element. For instance, Tesseract will output bounding boxes for each word
detected on a page, in the [HOCR](https://en.wikipedia.org/wiki/HOCR) format
which also nests words within lines, paragraphs, and blocks.

In situtations like this, some information is encoded in each bounding box --
the size of the box, the text of the word it contains, the position of the box
on the page. But a lot of information is also contained in the spatial
relationships among the bounding boxes. It's helpful to think of the layout as
a graph. Every bounding box is a node, and we draw an edge between two nodes
whenever they are *adjacent* to one another, as defined by some similarity
condition. In the following snippet, `boxes2edges` takes a data frame of
bounding boxes along with a suitable definition of adjacency and returns the
edges in the resulting graph. `add_groupid` builds on that foundation, pulling
out connected components from the layout graph.

```{r, warning=FALSE, message=FALSE}
pacman::p_load(tidyverse, tidygraph)

boxes2edges <- function(boxes, similarity, threshold) {
    n_box <- nrow(boxes)
    result <- vector("list", n_box)
    for (index in seq_len(n_box-1)) {
        ind2 <- index+1
        box1 <- boxes[index, ]
        boxk <- boxes[ind2:n_box, ]
        sim <- similarity(box1, boxk)
        neighbor_ids <- boxk$id[sim > threshold]
        if (length(neighbor_ids) > 0)
            result[[index]] <- tibble(from=box1$id, to=neighbor_ids)
    }
    bind_rows(purrr::compact(result))
}

add_groupid <- function(boxes, ...) {
    edges <- boxes2edges(boxes, ...)
    tbl_graph(edges=mutate_all(edges, as.character),
              nodes=boxes,
              directed=F) %>%
    mutate(groupid=group_components()) %>%
    as_tibble("nodes")
}
```

You can get pretty far by defining `similarity` in terms of Euclidean distance,
but there are other metrics that are especially well-suited for page-layouts.
Consider this relatively simple example, a region identified by our
structure-extraction tools as a table header followed by table rows:

```{r, echo=FALSE}
gg(boxes) +
    geom_rect(fill="grey90", color="black", size=.2)
```

In order to recover the column structure, I can create my bounding box graph
using overlap in the x dimension.

```{r}
overlap <- function(a.start, a.end, b.start, b.end) {
    unadjusted <- pmin(a.end, b.end) - pmax(a.start, b.start)
    unadjusted <- pmax(0, unadjusted)
    max_overlap <- pmax(a.end-a.start, b.end-b.start)
    unadjusted/max_overlap
}

x_overlap = function(box1, box2) {
    overlap(box1$x0, box1$x1, box2$x0, box2$x1)
}

columns <- boxes %>%
    add_groupid(similarity=x_overlap, threshold=0) %>%
    mutate(column=letters[groupid]) %>%
    group_by(column) %>%
    summarise(x0=min(x0), x1=max(x1), y0=min(y0), y1=max(y1),
              .groups="drop")

```

The resulting groupings nicely group data into columns. If rows are multi-line,
we can use similar logic to identify rows.

```{r, echo=FALSE}
gg(boxes) +
    geom_rect(data=columns, color="black", fill=NA) +
    geom_rect(fill="grey90", color="black", size=.2)
```
